import datetime
import sys
import warnings
from os.path import abspath, join, dirname
from textwrap import dedent
top_level_directory = abspath(join(dirname(__file__), ".."))
sys.path.insert(0, top_level_directory)

import simplejson as json
from hdfs3 import HDFileSystem

from conf import CONFIG
from templater import Templater
from constants import FOOTER_PATTERN
from constants import HADOOP_PATH
from constants import MESSAGE_VERSION
from constants import SETTING
from constants import TEMPLATE_PATH
from pipeline_message_example import PIPELINE_MESSAGE
from logger import custom_logger

# Initialize logger
logger = custom_logger()
warnings.filterwarnings("ignore")

print ("top_level_directory : ", top_level_directory)
print ("HADOOP_PATH : ", HADOOP_PATH)


def fixing_indentation(inp_str):
    """
    Fix indentation and clean unused line in inp_str.
    example:
    before fixing indentation
    ---------------------------------------------------------------
        #!/usr/bin/env python3
        # -*- coding: utf-8 -*-
        '''
        This code generated by running_executable_code.py made by: .
        Author: @author
        '''
        #==============================================================
        # Type code: Fit by Cross Validation
        #==============================================================
    ---------------------------------------------------------------
    after fixing indentation
    ---------------------------------------------------------------
    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-
    '''
    This code generated by running_executable_code.py made by: .
    Author: @author
    '''
    #==============================================================
    # Type code: Fit by Cross Validation
    #==============================================================
    ---------------------------------------------------------------
    :param inp_str: (str) part of the executable code will be writen
                    in file .py
    :return: (str) string formatted
    """

    # clean unused line at begining and end
    clean_str = inp_str[inp_str.find("\n") + 1:inp_str.rfind("\n")]
    # fix indentation
    clean_str = dedent(clean_str)

    return clean_str


def write_import_modul(estimator, pipeline_type, message):
    """
    Write import all modul from estimator and/or evaluator.
    :param estimator: (dict) list of estimator from user pipeline
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :param message: (list) message of current pipeline
    :return: (str) modul imported in string
    """

    # get modul dict
    modul_dict = SETTING["MODUL_DICT"]

    # Import all modul dependencies
    # Import evaluator in certain pipeline
    modul_imported = "\n"
    modul_imported += "from pyspark.ml import Pipeline\n"
    if pipeline_type != 0 and pipeline_type != 3:
        modul_imported += "from pyspark.ml.tuning import ParamGridBuilder\n"
        if pipeline_type == 1:
            # Cross Validator
            modul_imported += "from pyspark.ml.tuning import CrossValidator\n"
        elif pipeline_type == 2:
            # Train Validation Split
            modul_imported += \
                "from pyspark.ml.tuning import TrainValidationSplit\n"

        # both Cross Validator or Train Validation Split evaluator
        evaluator_modul = message["evaluator"]["name"]
        modul_imported += modul_dict[evaluator_modul] + "\n"

    # import estimator from each pipeline type
    import_statement = []
    for stg in range(len(estimator)):
        modul = list(estimator[stg].keys())[0]
        import_modul_str = modul_dict[modul]
        if import_modul_str not in import_statement:
            import_statement.append(import_modul_str)
            modul_imported += import_modul_str + "\n"

    modul_imported += "\n\n"

    return modul_imported


def write_stage_result_function_definition(pipeline_type):
    """
    Define a function stage_result and description,
    only define a function
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :return: (str) stage_result function name and description in string
    """

    output = ""
    if pipeline_type == 0:
        output = "Pipeline model."
    elif pipeline_type == 1:
        output = "The best CV pipeline model."
    elif pipeline_type == 2:
        output = "The TVS best pipeline model."
    elif pipeline_type == 3:
        output = "Transformed dataset."

    stage_result = f"""
        def stage_result(data_train, message):
        \t\"\"\"
        \tInput: Dataset
        \tOutput: {output}
        \t\"\"\"
        """

    stage_result = fixing_indentation(stage_result)

    return stage_result


def write_algorithm_object(estimator):
    """
    Write all algorithm object is needed in the pipeline
    and modul list for pipeline
    example:
        params_dict_MinMaxScaler = {
                "inputCol": "fitur", "outputCol": "fiturMinMaxScaler",
                "min": 0, "max": 1
                }
        modul_MinMaxScaler = MinMaxScaler(**params_dict_MinMaxScaler)
        params_dict_GeneralizedLinearRegression = {
                "featuresCol": "fiturMinMaxScaler",
                "labelCol": "price_volume", "predictionCol": "prediction",
                "linkPredictionCol": "link_prediction", "solver": "irls",
                "fitIntercept": True, "family": "tweedie", "maxIter": 100,
                "tol": 1e-06, "regParam": 0.01, "variancePower": 0, "link":
                "identity", "linkPower": 2
                }
        modul_GeneralizedLinearRegression =
            GeneralizedLinearRegression(**params_dict_GeneralizedLinearRegression)
        stages_pipeline1 = [modul_MinMaxScaler, modul_GeneralizedLinearRegression]
    :param estimator: (dict) list of estimator from user pipeline
    :return: (str)
    """

    stages_code_string = ""
    strung = ""
    modul_list = []

    # writing all algorithm objects estimator with their parameters
    for stg in range(len(estimator)):
        # modul_i adalah key dari dictionary dalam list estimator stages ke stg
        modul_i = list(estimator[stg].keys())[0]
        if modul_i != "PipelineModel":
            # create index for modul_i if modul_i exist in modul_list
            # example modul_GeneralizedLinearRegression_1
            algo_class = modul_i
            if modul_i in modul_list:
                modul_list.append(modul_i)
                modul_number = "_" + str(modul_list.count(modul_i))
                modul_i += modul_number
            else:
                modul_list.append(modul_i)

            # extract all parameters of modul_i
            params_dict = estimator[stg][algo_class]
            # writing paramemers of modul_i
            string = "\tparams_dict_" + modul_i + " = " + str(params_dict) + "\n"
            # writing object of modul_i with parameters
            strong = "\tmodul_" + modul_i + " = " + algo_class + "(**params_dict_" \
                     + modul_i + ")\n\n"
            strung += string + strong
            # create pipeline from modul_i
            stages_code_string += "modul_" + modul_i + \
                                  (", " if stg != len(estimator) - 1 else "")
        else:
            # PipelineModel condition used to process existing model
            path_model = estimator[stg][modul_i]["path"]
            modul_id = str(estimator[stg][modul_i]["id"])
            modul_id = modul_id.replace("-", "_")
            streng = "\tmodul_" + modul_id + " = " + modul_i + \
                     ".read().load('" + path_model + "')\n\n"
            strung += streng
            stages_code_string += "modul_" + modul_id + \
                                  (", " if stg != len(estimator) - 1 else "")

    # create pipeline from moduls
    stages_code_string = "\tstages_pipeline" + \
                         " = [" + stages_code_string + "]\n\n"

    return strung + stages_code_string


def write_prediction_process(pipeline_type):
    """
    Write prediction process each pipeline type.
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :return: (str)
    """

    # prediction process
    pred_process = "\tif message['output_col'] == 'all':\n"
    pred_process += "\t\tdata_prediction = prediction\n"
    pred_process += "\telse:\n"
    pred_process += "\t\ttry:\n"
    pred_process += "\t\t\toutput_col = set(message['output_col'])\n"
    pred_process += "\t\t\tprediction_col = set(prediction.columns)\n"
    pred_process += "\t\t\toutput_col = list(output_col.intersection(prediction_col))\n"
    pred_process += "\t\t\tdata_prediction = prediction.select([col for col in prediction" \
        ".columns if col in output_col])\n"
    pred_process += "\t\texcept Exception as er:\n"
    pred_process += "\t\t\traise er\n\n"

    # case for general pipeline and transform without model evaluator
    if pipeline_type == 0 or pipeline_type == 3:
        temp = "\tpipeline = Pipeline(stages=stages_pipeline)\n"
        temp += "\tmodel = pipeline.fit(data_train)\n\n"
        temp += "\tprediction = model.transform(data_train)\n"
        pred_process = temp + pred_process

    return pred_process


def write_evaluation_process(message, pipeline_type):
    """
    Write evaluation process each pipeline type.
    :param message: (list) message of current pipeline
    :param pipeline_type: (int)
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
    :return: (str)
    """

    evaluator_metric_name = message["evaluator"]["params"]["metricName"]

    # Cross Validator
    if pipeline_type == 1:
        num_fold_or_train_ratio = message["numFolds"]
        param = "numFolds"
        obj_name = "crossval"
        eval_name = "CrossValidator"
        eval_title = "cross-validation"
        model_name = "model_cv"
    # Train Validation Split
    else:
        num_fold_or_train_ratio = message["trainRatio"]
        param = "trainRatio"
        obj_name = "tvs"
        eval_name = "TrainValidationSplit"
        eval_title = "train-validation-split"
        model_name = "model_tvs"

    # writing evaluation process
    params_evaluator = "params_evaluator = " + \
                       str(message["evaluator"]["params"]) + "\n"

    # if error use this for TVS
    modul_evaluator = "modul_evaluator = " + str(message["evaluator"]["name"]) \
                      + "(**params_evaluator)" + "\n\n"

    eval_process = "\t" + params_evaluator
    eval_process += "\t" + modul_evaluator
    eval_process += "\t" + obj_name + " = " + eval_name \
        + "(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=modul_evaluator, " \
        + param + "=" + str(num_fold_or_train_ratio) + ")\n\n"
    eval_process += "\t# Run " + eval_title + \
        ", and choose the best set of parameters.\n\n"
    eval_process += "\t" + model_name + " = " + obj_name + ".fit(data_train)\n\n"
    eval_process += "\tevaluator = modul_evaluator\n\n"
    eval_process += "\tprediction = " + model_name + ".transform(data_train)\n\n"
    eval_process += "\tevaluator_value = evaluator.evaluate(prediction)\n\n"
    eval_process += "\tbest_model = " + model_name + ".bestModel\n\n"
    eval_process += "\tmetric_performance = {'" + evaluator_metric_name + \
        "': float(evaluator_value)}\n\n"

    return eval_process


def write_param_grid(message):
    """
    Write param grid each pipeline type.
    :param message: (list) message of current pipeline
    :return: (str)
    """

    # extract param grid text
    estimator_param_map = \
        message["estimatorParamMaps"]["ParamGridBuilder"]
    param_grid_text = "\tparamGrid = ParamGridBuilder()"
    modul_param_list = []
    for est in range(len(estimator_param_map)):
        est_key = list(estimator_param_map[est].keys())[0]
        est_key_num = est_key
        if est_key in modul_param_list:
            modul_param_list.append(est_key)
            modul_number = "_" + str(modul_param_list.count(est_key))
            est_key_num += modul_number
        else:
            modul_param_list.append(est_key)
        if estimator_param_map[est][est_key] != "None":
            param_builder_keys = list(estimator_param_map[est][est_key].keys())
            param_builder_dict = estimator_param_map[est][est_key]
            for param_key in range(len(param_builder_keys)):
                param_builder_key = param_builder_keys[param_key]
                param_builder_value = str(param_builder_dict[param_builder_key])
                param_builder_key_value = param_builder_key + ", " \
                    + param_builder_value + ")"
                param_grid_text = param_grid_text + ".addGrid(modul_" \
                    + est_key_num + "." + param_builder_key_value

    return param_grid_text


def code_generator_fit_gp(estimator, filename):
    """
    Generate pyspark executable code spark pipeline without evaluator.
    :param estimator: (dict) list of estimator from user pipeline
    :param filename: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 0
    try:
        # Importing modul
        import_modul_str = write_import_modul(estimator, pipeline_type, {})

        # Creating stage result function definition
        stage_result_str = write_stage_result_function_definition(
            pipeline_type)

        # Creating algorithm object
        algo_object_str = write_algorithm_object(estimator)

        # Creating prediction process
        pred_process_str = write_prediction_process(pipeline_type)

        # Creating return statement
        return_str = "\treturn model, data_prediction\n\n\n"

        # Join all together
        content = import_modul_str + stage_result_str + algo_object_str \
            + pred_process_str + return_str

        # Append to script file
        file = open(filename, "a")
        file.write(content)
        file.close()

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def code_generator_fit_cv(message, estimator, filename):
    """
    Generate pyspark executable code spark pipeline with Cross Validator.
    :param message: (list) message of current pipeline
    :param estimator: (dict) list of estimator from user pipeline
    :param filename: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 1
    try:
        # Importing modul
        import_modul_str = write_import_modul(
            estimator, pipeline_type, message)

        # Creating stage result function definition
        stage_result_str = write_stage_result_function_definition(
            pipeline_type)

        # Creating algorithm object
        algo_object_str = write_algorithm_object(estimator)
        algo_object_str += "\tpipeline = Pipeline(stages=stages_pipeline)\n\n"

        # Extract param grid text and join to header
        param_grid_str = write_param_grid(message)
        param_grid_str += ".build()\n\n"

        # Create evaluation process and join to header
        eval_process_str = write_evaluation_process(message, pipeline_type)

        # Create prediction process
        pred_process_str = write_prediction_process(pipeline_type)

        # Creating return statement
        return_str = "\treturn best_model, metric_performance, data_prediction\n\n\n"

        # Join all together
        content = import_modul_str + stage_result_str + algo_object_str + param_grid_str \
            + eval_process_str + pred_process_str + return_str

        # Append to script file
        file = open(filename, "a")
        file.write(content)
        file.close()

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def code_generator_fit_tvs(message, estimator, filename):
    """
    Generate pyspark executable code spark pipeline with Train Validation Split.
    :param message: (list) message of current pipeline
    :param estimator: (dict) list of estimator from user pipeline
    :param filename: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 2
    try:
        # Importing modul
        import_modul_str = write_import_modul(
            estimator, pipeline_type, message)

        # Creating stage result function definition
        stage_result_str = write_stage_result_function_definition(
            pipeline_type)

        # Creating algorithm object
        algo_object_str = write_algorithm_object(estimator)
        algo_object_str += "\tpipeline = Pipeline(stages=stages_pipeline)\n\n"

        # Extract param grid text
        param_grid_str = write_param_grid(message)
        param_grid_str += ".build()\n\n"

        # Create evaluation process
        eval_process_str = write_evaluation_process(message, pipeline_type)

        # Create prediction process
        pred_process_str = write_prediction_process(pipeline_type)

        # Creating return statement
        return_str = "\treturn best_model, metric_performance, data_prediction\n\n\n"

        # Join all together
        content = import_modul_str + stage_result_str + algo_object_str \
            + param_grid_str + eval_process_str + pred_process_str + return_str

        # Append to script file
        file = open(filename, "a")
        file.write(content)
        file.close()

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def code_generator_transform(estimator, filename):
    """
    Generate pyspark executable code spark pipeline without evaluator.
    :param estimator: (dict) list of estimator from user pipeline
    :param filename: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 3
    try:
        # Importing modul
        import_modul_str = write_import_modul(
            estimator, pipeline_type, {})

        # Creating stage result function definition
        stage_result_str = write_stage_result_function_definition(
            pipeline_type)

        # Creating algorithm object
        algo_object_str = write_algorithm_object(estimator)

        # Creating prediction process
        pred_process_str = write_prediction_process(pipeline_type)

        # Creating return statement
        return_str = "\treturn model, data_prediction\n\n\n"

        # Join all together
        content = import_modul_str + stage_result_str + algo_object_str \
            + pred_process_str + return_str

        # Append to script file
        file = open(filename, "a")
        file.write(content)
        file.close()

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def generate_script_name(pipeline_id):
    """
    Generate filename.
    :param pipeline_id: (str) pipeline id
    :return:
    """

    date_now = datetime.datetime.utcnow()
    date_format = "%Y%m%d%H%M%S%f"
    date_now_formatted = datetime.datetime.strftime(date_now, date_format)
    pipeline_id = pipeline_id.replace("-", "_")
    filename = "C{0}_{1}_main".format(date_now_formatted, pipeline_id)

    return filename


def create_header(message, filename):
    """
    Create header script.
    :param message: (dict) pipeline parameters
    :param filename: (str) filename with absolute path
    :return:
    """

    try:
        header_template_path = "{0}{1}/header.txt".format(top_level_directory, TEMPLATE_PATH["TEMPLATE"])
        logger.info("header_template_path : '{0}'.".format(header_template_path))
        header_pattern = {
            "$SPARK_CODE_DEPEDENCIES_URL": CONFIG["SPARK_CODE_DEPEDENCIES_URL"],
            "$PIPELINE": str(message)
        }
        #print ('header_pattern : ', header_pattern)
        header_str = Templater.load_template(header_template_path)
        header_str = Templater.replace_word(header_str, header_pattern)

        Templater.save_as(header_str, filename)

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def create_body(message, filename):
    """
    Create body script.
    :param message: (dict) pipeline parameters
    :param filename: (str) filename with absolute path
    :return:
    """

    try:
        estimator = message["estimator"]

        if message["last_stage"] == "fit":
            if message["type"] == "GeneralPipeline":
                code_generator_fit_gp(estimator, filename)
            elif message["type"] == "CrossValidator":
                code_generator_fit_cv(message, estimator, filename)
            elif message["type"] == "TrainValidationSplit":
                code_generator_fit_tvs(message, estimator, filename)
            else:
                raise KeyError("Unknown key 'type' in message.")

        elif message["last_stage"] == "transform":
            code_generator_transform(estimator, filename)
        else:
            raise KeyError("Unknown key 'last_stage' in message.")

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def create_footer(message, filename):
    """
    Create footer script.
    :param message: (dict) pipeline parameters
    :param filename: (str) filename with absolute path
    :return:
    """

    try:
        footer_template_path = "{0}{1}/footer.txt".format(top_level_directory, TEMPLATE_PATH["TEMPLATE"])

        if message["last_stage"] == "fit":
            if message["type"] == "GeneralPipeline":
                footer_pattern = FOOTER_PATTERN["GP"]
            elif message["type"] == "CrossValidator":
                footer_pattern = FOOTER_PATTERN["CV"]
            elif message["type"] == "TrainValidationSplit":
                footer_pattern = FOOTER_PATTERN["TVS"]
            else:
                raise KeyError("Unknown key 'type' in message.")

        elif message["last_stage"] == "transform":
            footer_pattern = FOOTER_PATTERN["TRANSFORM"]
        else:
            raise KeyError("Unknown key 'last_stage' in message.")

        footer_str = Templater.load_template(footer_template_path)
        footer_str = Templater.replace_word(footer_str, footer_pattern)

        Templater.save_as(footer_str, filename, "a")

    except FileExistsError as fee:
        raise fee
    except FileNotFoundError as fnfe:
        raise fnfe
    except Exception as er:
        raise er

    return True


def generate_script(message: dict, filename: str):
    """
    Generate executable code from message with filename.
    :param message: (dict) pipeline parameters
    :param filename: (str) filename with absolute path
    :return:
    """

    try:
        logger.info("Generating python script.")
        create_header(message, filename)
        create_body(message, filename)
        create_footer(message, filename)
        logger.info("Script is successfully created with name '{0}'.".format(filename))

    except Exception as er:
        logger.error("Failed to generate python script.")
        logger.exception("Error:{0}".format(str(er)))
        raise er

    return True


def generate_path(user_id: str, pipeline_id: str, pipeline_type: str):
    """
    Generating hadoop path for model or data and metadata.
    :param user_id: (str)
    :param pipeline_id: (str)
    :param pipeline_type: (str)
    :return:
    """

    if pipeline_type == "FIT":
        pipeline_result_path = "{0}/{1}/{2}".format(HADOOP_PATH["MODEL"], user_id, pipeline_id)
    else: #TRANSFORM
        pipeline_result_path = "{0}/{1}/{2}".format(HADOOP_PATH["DATA"], user_id, pipeline_id)

    metadata_path = "{0}/{1}/{2}".format(HADOOP_PATH["METADATA"], user_id, pipeline_id)

    return pipeline_result_path, metadata_path


def get_algorithm_configuration(message):
    """
    Get algorithm configuration from message
    and add the id, hadoop path for model and metadata.
    :param message: (dict)
    :return:
    """

    user_id = message["userId"]
    pipeline_id = message["id"]
    pipeline_type = message["pipelineType"]
    pipeline_result_path, metadata_path = generate_path(user_id, pipeline_id, pipeline_type)

    # extract algorithm configuration from message
    algo_conf = json.loads(message["algorithmConfiguration"])

    # add to algorithm configuration
    algo_conf["id"] = pipeline_id
    algo_conf["pipeline_result_path"] = pipeline_result_path
    algo_conf["metadata_path"] = metadata_path
    algo_conf["spark_job_version"] = MESSAGE_VERSION

    return algo_conf


def upload_script_to_hadoop(hdfs: HDFileSystem, local_script: str, hadoop_filename: str):
    """
    Upload the generated script to hadoop.
    :param hdfs: (HDFileSystem)
    :param local_script: (str) path to file
    :param hadoop_filename: (str)
    :return:
    """

    try:
        hadoop_script = "{0}/{1}.py".format(HADOOP_PATH["SCRIPT"], hadoop_filename)
        hdfs.put(local_script, hadoop_script)
        logger.info("Script '{0}.py' is successfully uploaded to hadoop.".format(hadoop_filename))

    except Exception as er:
        logger.error("Failed occured when uploading script to hadoop.")
        logger.exception("Error: {0}".format(str(er)))
        raise er

    return True


def remove_script(filename: str):
    """
    Upload script to hadoop.
    :param filename: (str)
    :return:
    """

    try:
        import os
        os.remove(filename)

    except Exception as er:
        logger.error("Failed occured when remove script.")
        logger.exception("Error: {0}".format(str(er)))
        raise er

    return True


def generator(message: dict):
    """
    Generate script for model or data with pyspark base template.
    :param message: (dict)
    :return:
    """

    try:
        algo_config = get_algorithm_configuration(message) #Get Algorithm Configuration from Message and add id to it

        filename = generate_script_name(message["id"]) #Generate name for the generated script
        script = "{0}{1}/{2}.py".format(top_level_directory, TEMPLATE_PATH["SCRIPT"], filename) #Script path or directory

        generate_script(algo_config, script)

        # create connection hadoop
        #hdfs = HDFileSystem()

        #upload_script_to_hadoop(hdfs, script, filename)
        #remove_script(script)

    except Exception as err:
        raise err

    return filename


def main():
    for pipeline in PIPELINE_MESSAGE[0:1]:
        generator(pipeline)


if __name__ == "__main__":
    main()
