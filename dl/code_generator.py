import datetime
from os.path import abspath, join, dirname, relpath
import subprocess
import sys
from textwrap import dedent
#top_level_directory = abspath(join(dirname(__file__), '..'))
#sys.path.insert(0, top_level_directory)

#from modules.running_info_writer import write_info
#from modules.update_status_and_produce_message import \
#    update_status_and_produce_message_fail_exit
from constants import SETTING
#from utils.kafka.kafka_producer import producer_response
from logger import custom_logger

logger = custom_logger()


def fixing_indentation(inp_str):
    """
    Fix indentation and clean unused line in inp_str.
    example:
    before fixing indentation
    ---------------------------------------------------------------
        #!/usr/bin/env python3
        # -*- coding: utf-8 -*-
        '''
        This code generated by running_executable_code.py made by: .
        Author: @author
        '''
        #==============================================================
        # Type code: Fit by Cross Validation
        #==============================================================
    ---------------------------------------------------------------
    after fixing indentation
    ---------------------------------------------------------------
    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-
    '''
    This code generated by running_executable_code.py made by: .
    Author: @author
    '''
    #==============================================================
    # Type code: Fit by Cross Validation
    #==============================================================
    ---------------------------------------------------------------
    :param inp_str: (str) part of the executable code will be writen
                    in file .py
    :return: (str) string formatted
    """

    # clean unused line at begining and end
    clean_str = inp_str[inp_str.find('\n') + 1:inp_str.rfind('\n')]
    # fix indentation
    clean_str = dedent(clean_str)

    return clean_str


def write_description_code_generator(pipeline_type):
    """
    Write header description of each pipeline type.
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :return: (str) header description
    """

    pipeline_title = ""
    if pipeline_type == 0:
        pipeline_title = "Fit by GeneralPipeline."
    elif pipeline_type == 1:
        pipeline_title = "Fit by Cross Validation."
    elif pipeline_type == 2:
        pipeline_title = "Fit by Train Validation Split."
    elif pipeline_type == 3:
        pipeline_title = "Transform by GeneralPipeline."

    header_description = f"""
        #!/usr/bin/env python3
        # -*- coding: utf-8 -*-
        \"\"\"
        This code generated by running_executable_code.py made by:
        Author: Mujirin, mujirin@volantis.io
        \"\"\"
        #==============================================================
        # Type code: {pipeline_title}
        #==============================================================
        """

    header_description = fixing_indentation(header_description)

    return header_description


def write_import_modul_dependencies_code_generator(
        mes, estimator, pipeline_type, message=[]):
    """
    Write import all modul from estimator and/or evaluator.
    :param mes: (int) index of message
    :param estimator: (dict) list of estimator from user pipeline
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :param message: (list) message of current pipeline
    :return: (str) modul imported in string
    """

    # get modul dict
    modul_dict = SETTING['MODUL_DICT']

    # Import all modul dependencies
    # Import evaluator in certain pipeline
    modul_imported = ""
    modul_imported += "from pyspark.ml import Pipeline\n"
    if pipeline_type != 0 and pipeline_type != 3:
        modul_imported += "from pyspark.ml.tuning import ParamGridBuilder\n"
        if pipeline_type == 1:
            # Cross Validator
            modul_imported += "from pyspark.ml.tuning import CrossValidator\n"
        elif pipeline_type == 2:
            # Train Validation Split
            modul_imported += \
                "from pyspark.ml.tuning import TrainValidationSplit\n"

        # both Cross Validator or Train Validation Split evaluator
        evaluator_modul = message[mes]['evaluator']['name']
        modul_imported += modul_dict[evaluator_modul] + '\n'

    # import estimator from each pipeline type
    import_statement = []
    for stg in range(len(estimator)):
        modul = list(estimator[stg].keys())[0]
        import_modul_str = modul_dict[modul]
        if import_modul_str not in import_statement:
            import_statement.append(import_modul_str)
            modul_imported += import_modul_str + '\n'

    return modul_imported


def write_function_stage_result_name_and_description(mes, pipeline_type):
    """
    Define a function stage_result and description,
    only define a function
    :param mes: (int) index of message
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :return: (str) stage_result function name and description in string
    """

    output = ""
    if pipeline_type == 0:
        output = "Pipeline model."
    elif pipeline_type == 1:
        output = "The best CV pipeline model."
    elif pipeline_type == 2:
        output = "The TVS best pipeline model."
    elif pipeline_type == 3:
        output = "Transformed dataset."

    stage_result = f"""
        def stage_result(data_train, message_{mes + 1}):
        \t\"\"\"
        \tInput: Dataset
        \tOutput: {output}
        \t\"\"\"
        """

    stage_result = fixing_indentation(stage_result)

    return stage_result


def write_algorithm_object(mes, estimator):
    """
    Write all algorithm object is needed in the pipeline
    and modul list for pipeline
    example:
        params_dict_MinMaxScaler = {
                'inputCol': 'fitur', 'outputCol': 'fiturMinMaxScaler',
                'min': 0, 'max': 1
                }
        modul_MinMaxScaler = MinMaxScaler(**params_dict_MinMaxScaler)
        params_dict_GeneralizedLinearRegression = {
                'featuresCol': 'fiturMinMaxScaler',
                'labelCol': 'price_volume', 'predictionCol': 'prediction',
                'linkPredictionCol': 'link_prediction', 'solver': 'irls',
                'fitIntercept': True, 'family': 'tweedie', 'maxIter': 100,
                'tol': 1e-06, 'regParam': 0.01, 'variancePower': 0, 'link':
                'identity', 'linkPower': 2
                }
        modul_GeneralizedLinearRegression =
            GeneralizedLinearRegression(**params_dict_GeneralizedLinearRegression)
        stages_pipeline1 = [modul_MinMaxScaler, modul_GeneralizedLinearRegression]
    :param mes: (int) index of message
    :param estimator: (dict) list of estimator from user pipeline
    :return: (str)
    """

    stages_code_string = ''
    strung = ''
    modul_list = []

    # writing all algorithm objects estimator with their parameters
    for stg in range(len(estimator)):
        # modul_i adalah key dari dictionary dalam list estimator stages ke stg
        modul_i = list(estimator[stg].keys())[0]
        if modul_i != 'PipelineModel':
            # create index for modul_i if modul_i exist in modul_list
            # example modul_GeneralizedLinearRegression_1
            algo_class = modul_i
            if modul_i in modul_list:
                modul_list.append(modul_i)
                modul_number = "_" + str(modul_list.count(modul_i))
                modul_i += modul_number
            else:
                modul_list.append(modul_i)

            # extract all parameters of modul_i
            params_dict = estimator[stg][algo_class]
            # writing paramemers of modul_i
            string = '\tparams_dict_' + modul_i + ' = ' + str(params_dict) + '\n'
            # writing object of modul_i with parameters
            strong = "\tmodul_" + modul_i + " = " + algo_class + "(**params_dict_" \
                     + modul_i + ")\n\n"
            strung += string + strong
            # create pipeline from modul_i
            stages_code_string += "modul_" + modul_i + \
                                  (", " if stg != len(estimator) - 1 else "")
        else:
            # PipelineModel condition used to process existing model
            path_model = estimator[stg][modul_i]['path']
            modul_id = str(estimator[stg][modul_i]['id'])
            modul_id = modul_id.replace("-", "_")
            streng = "\tmodul_" + modul_id + " = " + modul_i + \
                     ".read().load('" + path_model + "')\n\n"
            strung += streng
            stages_code_string += "modul_" + modul_id + \
                                  (", " if stg != len(estimator) - 1 else "")

    # create pipeline from moduls
    stages_code_string = '\tstages_pipeline' + str(mes + 1) + \
                         ' = [' + stages_code_string + ']\n\n'

    return strung + stages_code_string


def write_prediction_process(mes, pipeline_type):
    """
    Write prediction process each pipeline type.
    :param mes: (int) index of message
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    :return: (str)
    """

    # prediction process
    pred1 = '\tif message_' + str(mes + 1) + '["output_col"] == "all":\n'
    pred2 = '\t\tdata_prediction = prediction' + str(mes + 1) + '\n'
    pred3 = '\telse:\n'
    pred4 = '\t\tn = 0\n'
    pred5 = '\t\twhile n < 3:\n'
    pred6 = '\t\t\ttry:\n'
    pred7 = '\t\t\t\toutput_col = set(message_' + str(mes + 1) \
            + '["output_col"])\n'
    pred8 = '\t\t\t\tprediction_col = set(prediction' + str(mes + 1) \
            + '.columns)\n'
    pred9 = '\t\t\t\toutput_col = list(output_col.intersection(prediction_col))\n'
    pred10 = '\t\t\t\tdata_prediction = prediction' + str(mes + 1) + \
             '.select([col for col in prediction' + str(mes + 1) + \
             '.columns if col in output_col])\n'
    pred11 = '\t\t\t\tbreak\n'
    pred12 = '\t\t\texcept Exception as e:\n'
    pred13 = '\t\t\t\tn = n + 1\n'
    pred14 = '\t\t\t\tdata_prediction = prediction' + str(mes + 1) + '\n'
    pred15 = '\t\t\t\tif n == 3:\n'
    pred16 = '\t\t\t\t\tprint("[stage_result] Filtering dataframe failed in 3 trial")\n'
    pred17 = '\t\t\t\t\traise e\n'

    # case for transform pipeline
    if pipeline_type == 3:
        pred10 += '\t\t\t\tprint("Data transformation succeded...........")\n'

    pred_process = pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 \
        + pred8 + pred9 + pred10 + pred11 + pred12 + pred13 + pred14 \
        + pred15 + pred16 + pred17

    # case for general pipeline and transform without model evaluator
    foot = ""
    if pipeline_type == 0 or pipeline_type == 3:
        foot += '\tpipeline' + str(mes + 1) + \
                ' = Pipeline(stages=stages_pipeline' + str(mes + 1) + ')\n'
        foot += '\tmodel' + str(mes + 1) + ' = pipeline' + \
                str(mes + 1) + '.fit(data_train)\n\n'
        foot += '\tprediction' + str(mes + 1) + ' = model' + str(mes + 1) + \
                '.transform(data_train)\n'
        foot += pred_process
        pred_process = foot

    return pred_process


def write_evaluation_process(message, mes, pipeline_type):
    """
    Write evaluation process each pipeline type.
    :param message: (list) message of current pipeline
    :param mes: (int) index of message
    :param pipeline_type: (int)
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
    :return: (str)
    """

    evaluator_metric_name = message[mes]['evaluator']['params']['metricName']

    # Cross Validator
    if pipeline_type == 1:
        num_fold_or_train_ratio = message[mes]['numFolds']
        param = 'numFolds'
        obj_name = "crossval"
        eval_name = "CrossValidator"
        eval_title = "cross-validation"
        model_name = "model_cv"
    # Train Validation Split
    else:
        num_fold_or_train_ratio = message[mes]['trainRatio']
        param = 'trainRatio'
        obj_name = "tvs"
        eval_name = "TrainValidationSplit"
        eval_title = "train-validation-split"
        model_name = "model_tvs"

    # writing evaluation process
    params_evaluator = 'params_evaluator = ' + \
                       str(message[mes]['evaluator']['params']) + "\n"

    # if error use this for TVS
    # modul_evaluator = 'modul_evaluator = '
    # + message[mes]['evaluator']['name']+"(**params_evaluator)" + "\n\n"
    modul_evaluator = 'modul_evaluator = ' + \
                      str(message[mes]['evaluator']['name']) + \
                      "(**params_evaluator)" + "\n\n"

    e1 = '\t' + params_evaluator
    e2 = '\t' + modul_evaluator
    foot2 = '\t' + obj_name + str(mes + 1) + ' = ' + eval_name + \
            '(estimator=pipeline' + str(mes + 1) + \
            ', estimatorParamMaps=paramGrid, evaluator=modul_evaluator, ' + \
            param + '=' + str(num_fold_or_train_ratio) + ')\n\n'
    foot3 = '\t# Run ' + eval_title + \
            ', and choose the best set of parameters.\n\n'
    foot4 = '\t' + model_name + str(mes + 1) + ' = ' + \
            obj_name + str(mes + 1) + '.fit(data_train)\n\n'
    foot5 = '\tevaluator' + str(mes + 1) + ' = modul_evaluator\n\n'
    foot6 = '\tprediction' + str(mes + 1) + ' = ' + model_name + \
            str(mes + 1) + '.transform(data_train)\n\n'
    foot8 = '\tevaluator_value' + str(mes + 1) + ' = evaluator' + \
            str(mes + 1) + '.evaluate(prediction' + str(mes + 1) + ')\n\n'
    foot9 = '\tbest_model = ' + model_name + str(mes + 1) + '.bestModel\n\n'
    foot10 = '\tmetric_performance = {"' + evaluator_metric_name + \
             '":float(evaluator_value' + str(mes + 1) + ')}\n\n'
    linep1 = '\tprint("Making ' + eval_title + \
             ' best model succeed..................................")\n\n'

    # Join all together
    eval_process = e1 + e2 + foot2 + foot3 + foot4 + foot5 + foot6 + foot8 \
        + foot9 + foot10 + linep1

    return eval_process


def write_param_grid(message, mes):
    """
    Write param grid each pipeline type.
    :param message: (list) message of current pipeline
    :param mes: (int) index of message
    :return: (str)
    """

    # extract param grid text
    estimator_param_map = \
        message[mes]['estimatorParamMaps']['ParamGridBuilder']
    param_grid_text = '\tparamGrid = ParamGridBuilder()'
    modul_param_list = []
    for est in range(len(estimator_param_map)):
        est_key = list(estimator_param_map[est].keys())[0]
        est_key_num = est_key
        if est_key in modul_param_list:
            modul_param_list.append(est_key)
            modul_number = "_" + str(modul_param_list.count(est_key))
            est_key_num += modul_number
        else:
            modul_param_list.append(est_key)
        if estimator_param_map[est][est_key] != "None":
            param_builder_keys = list(estimator_param_map[est][est_key].keys())
            param_builder_dict = estimator_param_map[est][est_key]
            for param_key in range(len(param_builder_keys)):
                param_builder_key = param_builder_keys[param_key]
                param_builder_value = str(param_builder_dict[param_builder_key])
                param_builder_key_value = param_builder_key + ", " \
                    + param_builder_value + ")"
                param_grid_text = param_grid_text + ".addGrid(modul_" \
                    + est_key_num + "." + param_builder_key_value

    return param_grid_text


def code_generator_fit_general_pipe(estimator, mes, nama_code):
    """
    Generate pyspark executable code spark pipeline without evaluator.
    :param estimator: (dict) list of estimator from user pipeline
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 0
    try:
        # Creating file
        file = open(nama_code + '_modul_' + str(mes + 1) + '.py', 'w')

        # Creating header description
        header_description = write_description_code_generator(pipeline_type)
        file.write(header_description)

        # Importing modul
        import_modul_str = write_import_modul_dependencies_code_generator(
            mes, estimator, pipeline_type)
        file.write(import_modul_str)

        # Getting stages result from message generator
        stage_result = write_function_stage_result_name_and_description(
            mes, pipeline_type)

        # Creating modul object
        stages_code_string = write_algorithm_object(mes, estimator)

        # Creating prediction process
        pred_process = write_prediction_process(mes, pipeline_type)

        # Creating return statement
        foot = '\treturn model' + str(mes + 1) + ', data_prediction\n'

        # Join all together
        header = stage_result + stages_code_string + pred_process + foot
    except Exception as er:
        logger.exception(er)

    code_generator_fit_general_pipe_content = header
    file.write(code_generator_fit_general_pipe_content)
    file.close()


def code_generator_fit_cv(message, estimator, mes, nama_code):
    """
    Generate pyspark executable code spark pipeline with Cross Validator.
    :param message: (list) message of current pipeline
    :param estimator: (dict) list of estimator from user pipeline
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 1
    try:
        # Creating file
        file = open(nama_code + '_modul_' + str(mes + 1) + '.py', 'w')
        header_description = write_description_code_generator(pipeline_type)
        file.write(header_description)

        # Creating header description
        import_modul_str = write_import_modul_dependencies_code_generator(
            mes, estimator, pipeline_type, message)
        file.write(import_modul_str)

        # Getting stages result from message generator
        stage_result = write_function_stage_result_name_and_description(
            mes, pipeline_type)

        # Creating modul object and pipeline
        stages_code_string = write_algorithm_object(mes, estimator)
        foot1 = '\tpipeline' + str(mes + 1) + ' = Pipeline(stages=stages_pipeline' \
                + str(mes + 1) + ')\n\n'
        strung = stages_code_string + foot1

        # Join
        header = stage_result + strung

        # Extract param grid text and join to header
        param_grid_text = write_param_grid(message, mes)
        param_grid_text += '.build()\n\n'
        header += param_grid_text

        # Create evaluation process and join to header
        eval_process = write_evaluation_process(message, mes, pipeline_type)
        all_code_generated = header + eval_process

        # Create prediction process
        pred_process = write_prediction_process(mes, pipeline_type)

        # Creating return statement
        foot11 = '\treturn best_model, metric_performance, data_prediction\n'

        # Join all together
        all_code_generated = all_code_generated + pred_process + foot11
    except Exception as er:
        logger.exception(er)

    code_generator_fit_cv_content = all_code_generated
    file.write(code_generator_fit_cv_content)
    file.close()


def code_generator_fit_tvs(message, estimator, mes, nama_code):
    """
    Generate pyspark executable code spark pipeline with Train Validation Split.
    :param message: (list) message of current pipeline
    :param estimator: (dict) list of estimator from user pipeline
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 2
    try:
        # Creating file
        file = open(nama_code + '_modul_' + str(mes + 1) + '.py', 'w')
        header_description = write_description_code_generator(pipeline_type)
        file.write(header_description)

        # Creating header description
        import_modul_str = write_import_modul_dependencies_code_generator(
            mes, estimator, pipeline_type, message)
        file.write(import_modul_str)

        # Getting stages result from message generator
        stage_result = write_function_stage_result_name_and_description(
            mes, pipeline_type)

        # Creating modul object and pipeline
        stages_code_string = write_algorithm_object(mes, estimator)
        foot1 = '\tpipeline' + str(mes + 1) + ' = Pipeline(stages=stages_pipeline' \
                + str(mes + 1) + ')\n\n'
        strung = stages_code_string + foot1

        # Join
        header = stage_result + strung

        # Extract param grid text and join to header
        param_grid_text = write_param_grid(message, mes)
        param_grid_text += '.build()\n\n'
        header = header + param_grid_text

        # Create evaluation process and join to header
        eval_process = write_evaluation_process(message, mes, pipeline_type)
        all_code_generated = header + eval_process

        # Create prediction process
        pred_process = write_prediction_process(mes, pipeline_type)

        # Creating return statement
        foot11 = '\treturn best_model, metric_performance, data_prediction\n'

        # Join all together
        all_code_generated = all_code_generated + pred_process + foot11
    except Exception as er:
        logger.exception(er)

    code_generator_fit_tvs_content = all_code_generated
    file.write(code_generator_fit_tvs_content)
    file.close()


def code_generator_transform(estimator, mes, nama_code):
    """
    Generate pyspark executable code spark pipeline without evaluator.
    :param estimator: (dict) list of estimator from user pipeline
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    """

    pipeline_type = 3
    try:
        # Creating file
        file = open(nama_code + '_modul_' + str(mes + 1) + '.py', 'w')

        # Creating header description
        header_description = write_description_code_generator(pipeline_type)
        file.write(header_description)

        # Importing modul
        import_modul_str = write_import_modul_dependencies_code_generator(
            mes, estimator, pipeline_type)
        file.write(import_modul_str)

        # Getting stages result from message generator
        stage_result = write_function_stage_result_name_and_description(
            mes, pipeline_type)

        # Creating modul object
        stages_code_string = write_algorithm_object(mes, estimator)

        # Creating prediction process
        pred_process = write_prediction_process(mes, pipeline_type)

        # Creating return statement
        foot = '\treturn data_prediction, model' + str(mes + 1) + '\n'

        # Join all together
        header = stage_result + stages_code_string + pred_process + foot
    except Exception as er:
        logger.exception(er)

    code_generator_transform_content = header
    file.write(code_generator_transform_content)
    file.close()


def import_modul(nama_main_code):
    """
    Import all modules in list_modul.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    """

    file = open(nama_main_code, 'a')
    list_modul = SETTING['LIST_MODUL']

    # writing import statement for all modules in list_modul
    for i in range(len(list_modul)):
        file.write(list_modul[i] + '\n')
    file.close()


def cluster_configuration(nama_main_code):
    """
    Generate or writing spark and cassandra configuration to the code generator.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    """

    # Spark Configuration
    file = open(nama_main_code, 'a')

    spark_conf = SETTING['SPARK_CONF']
    cassandra_conf = SETTING['CASSANDRA_CONF']
    kafka_conf = SETTING['KAFKA_CONF']

    app_name = spark_conf["app_name"]
    master = spark_conf["master"]
    cores = spark_conf["cores"]
    memory = spark_conf["memory"]
    spark_log_level = spark_conf["spark_log_level"]
    bind_address = spark_conf["bindAddress"]

    # Writing spark configurations to the file nama_main_code
    file.write('spark_settings = [("spark.app.name", ' + app_name
               + '),("spark.master", ' + master
               + '),("spark.cores.max", ' + cores
               + '),("spark.executor.memory", ' + memory
               + '),("spark.driver.bindAddress",' + bind_address
               + ')]\n')
    file.write('conf = SparkConf().setAll(spark_settings)\n')
    file.write('sc = SparkContext(conf=conf)\n')
    file.write('sc.setLogLevel(' + spark_log_level + ')\n')
    file.write('spark = SparkSession(sc).builder.getOrCreate()\n')
    file.write('spark_job_id = str(spark.sparkContext.applicationId)\n')
    file.write('print("Spark JOB ID:",spark_job_id)\n')

    # Cassandra database Configuration
    file.write('# Cassandra database configuration from write and '
               'update running history\n')
    cassandra_conf_fit = cassandra_conf['fit']
    cassandra_conf_transform = cassandra_conf['transform']

    # Writing cassandra configurations to the file nama_main_code
    file.write('host_fit = ' + cassandra_conf_fit['host'] + '\n')
    file.write('port_fit = ' + cassandra_conf_fit['port'] + '\n')
    file.write('host_transform = ' + cassandra_conf_transform['host'] + '\n')
    file.write('port_transform = ' + cassandra_conf_transform['port'] + '\n')
    file.write('database_name_fit = ' + cassandra_conf_fit['database_name'] + '\n')
    file.write('table_name_fit = ' + cassandra_conf_fit['table_name'] + '\n')
    file.write('database_name_transform = '
               + cassandra_conf_transform['database_name'] + '\n')
    file.write('table_name_transform = '
               + cassandra_conf_transform['table_name'] + '\n')

    # Kafka configuration for producing massage failed or done to topic = status
    file.write('# Kafka configuration for producing message failled '
               'or done to topic = status\n')
    file.write('host_kafka = ' + kafka_conf['host_kafka'] + '\n')

    # Pipeline configuration
    file.write('\n')
    file.close()


def insert_message(nama_main_code, message):
    """
    Write message variable to the main code or nama_main_code.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    :param message: (list) message of current pipeline
    """

    # Writing message to the nama_main_code
    file = open(nama_main_code, 'a')
    file.write('# Message\n')
    file.write('message = ' + str(message) + '\n')
    file.close()


def main_generated_and_see_result(message_i, nama_main_code):
    """
    Write Main generated code and result just for validation.
    :param message_i: (dict) message index i
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    """

    # create file
    file = open(nama_main_code, 'a')
    line7 = ""
    line10 = ""
    line11 = ""
    line20 = ""
    line21 = ""

    # set certain line of statement according to the type of pipeline
    if message_i["last_stage"] == 'fit':
        if message_i['type'] == "GeneralPipeline":
            line7 = '\t\t\tmodel, data_prediction, data_train, path, ' \
                    'time_total = get_result(message)'
            metric_performance = str({})
            line10 = '\t\t\tfrom modules.update_status_and_produce_message ' \
                     'import update_status_and_produce_message_done_fit_exit'
            line11 = '\t\t\tupdate_status_and_produce_message_done_fit_exit(' \
                     'data_prediction, data_train, path, time_total, message, model, ' \
                     + metric_performance + ', table_name_fit, spark_job_id)'
        elif message_i['type'] in ["CrossValidator", "TrainValidationSplit"]:
            line7 = '\t\t\tdata_prediction, data_train, path, time_total, ' \
                    'model, metric_performance = get_result(message)'
            line10 = '\t\t\tfrom modules.update_status_and_produce_message ' \
                     'import update_status_and_produce_message_done_fit_exit'
            line11 = '\t\t\tupdate_status_and_produce_message_done_fit_exit(' \
                     'data_prediction, data_train, path, time_total, message, ' \
                     'model, metric_performance, table_name_fit, spark_job_id)'
        line20 = '\t\t\t\tfrom modules.update_status_and_produce_message ' \
                 'import update_status_and_produce_message_fail_exit'
        line21 = '\t\t\t\tupdate_status_and_produce_message_fail_exit(' \
                 'message, path, table_name_fit, spark_job_id, e, 0)'
    elif message_i["last_stage"] == "transform":
        line7 = '\t\t\tdata_prediction, data_train, path, time_total = ' \
                'get_result(message)'
        line10 = '\t\t\tfrom modules.update_status_and_produce_message ' \
                 'import update_status_and_produce_message_done_transform_exit'
        line11 = '\t\t\tupdate_status_and_produce_message_done_transform_exit(' \
                 'data_prediction, data_train, path, time_total, message, ' \
                 'table_name_transform, spark_job_id)'
        line20 = '\t\t\t\tfrom modules.update_status_and_produce_message ' \
                 'import update_status_and_produce_message_fail_exit'
        line21 = '\t\t\t\tupdate_status_and_produce_message_fail_exit(' \
                 'message, path, table_name_transform, spark_job_id, e, 1)'
    else:
        line10 = ""
        line11 = ""

    # Writing main generated and result
    main_generated_and_see_result_str = f"""
    # Main generated and see result.
    def main(message):
    \trunning_number = 1
    \twhile running_number < 4:
    \t\ttry:
    {line7}
    \t\t\tprint("The pipeline had been trying ", running_number," times and DONE.")
    \t\t\t# Update status and producing message.
    {line10}
    {line11}
    \t\t\tDeleteFile.delete('{nama_main_code}')
    \t\t\tbreak
    \t\texcept Exception as e:
    \t\t\ttraceback.print_exc(file=sys.stdout)
    \t\t\trunning_number = running_number + 1
    \t\t\tif running_number == 4:
    \t\t\t\te = str(e)
    \t\t\t\tprint("Running the pipeling had been trying in 4 times but failed.")
    \t\t\t\tpath = ""
    {line20}
    {line21}
    \tspark.stop()
    if __name__ == "__main__":
    \tmain(message)
    """

    main_generated_and_see_result_content = \
        fixing_indentation(main_generated_and_see_result_str)
    file.write(main_generated_and_see_result_content)
    file.close()


def insert_data(nama_main_code, mes):
    """
    Inserting insert data code to the generated code
    from laniakea database url/API.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    :param mes: (int) index of message
    """

    file = open(nama_main_code, 'a')

    # Data train example
    file.write('\t\t# Inserting data.\n')
    file.write('\t\tdata_train = get_and_join_data(message_'
               + str(mes + 1) + ', spark)\n')
    file.close()
    # catatan: seandainya data ada dua dan di merge, atau
    # data hanya satu tapi di slice, atau di filter


def running_code(message_i, nama_main_code, mes, nama_code, pipeline_type):
    """
    Generating running code.
    :param message_i: (dict) message index i
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    """

    file = open(nama_main_code, 'a')

    hadoop_path_model = SETTING['HADOOP_PATH_MODEL']
    hadoop_path_data = SETTING['HADOOP_PATH_DATA']

    # Create indetify each pipeline type
    pipeline_title = ""
    if pipeline_type == 0:
        pipeline_title = "Fitting pipeline model"
    elif pipeline_type == 1:
        pipeline_title = "Fitting cross validation model"
    elif pipeline_type == 2:
        pipeline_title = "Fitting train validation split model"
    elif pipeline_type == 3:
        pipeline_title = "Data transformation"

    # Define return result variable of each pipeline type
    if pipeline_type == 0:
        pipeline_result = "model, data_prediction"
    elif pipeline_type == 3:
        pipeline_result = "data_prediction, model"
    else:
        pipeline_result = "model, metric_performance, data_prediction"

    # cut fullpath of nama_code and only use name_code without fullpath
    nama_code = relpath(nama_code, top_level_directory + '/code_generated/')

    # Importing modul for running
    file.write('\t#======================================================\n')
    file.write('\t# ' + pipeline_title + ' message ke-' + str(mes + 1) + '.\n')
    file.write('\t#======================================================\n')
    file.write('\t# Main running Code.\n')
    file.write('\t# Message ke-' + str(mes + 1) + ': ' + pipeline_title + '.\n')
    file.write('\ttry:\n')
    file.write('\t\tfrom ' + nama_code + '_modul_' + str(mes + 1)
               + ' import stage_result\n')

    # Preparing parameters for running
    file.write('\t\tmessage_' + str(mes + 1) + ' = message[' + str(mes) + ']\n')

    # Inserting hasil preparation
    file.close()

    # Data
    file = open(nama_main_code, 'a')
    insert_data(nama_main_code, mes)
    file.close()

    # Running
    file = open(nama_main_code, 'a')
    file.write('\t\ttime_start' + str(mes + 1) + ' = timeit.default_timer()\n')
    file.write('\t\tn = 0\n')
    file.write('\t\twhile n < 3:\n')
    file.write('\t\t\ttry:\n')
    file.write('\t\t\t\t' + pipeline_result + ' = stage_result(data_train, message_'
               + str(mes + 1) + ')\n')
    file.write('\t\t\t\tbreak\n')
    file.write('\t\t\texcept Exception as e:\n')
    file.write('\t\t\t\tn = n + 1\n')
    file.write('\t\t\t\tif n == 3:\n')
    file.write('\t\t\t\t\tprint("[stage_result] Training had been trying in ", '
               'n + 1, "trial failed.")\n')
    file.write('\t\t\t\t\traise e\n')
    file.write('\t\ttime_stop' + str(mes + 1) + ' = timeit.default_timer()\n')
    file.write('\t\ttime_total = time_stop' + str(mes + 1) + ' - time_start'
               + str(mes + 1) + '\n')
    if message_i["last_stage"] == "fit":
        file.write('\t\tfrom utils.hadoop.save_to_hadoop import save_model_to_hadoop\n')
        file.write('\t\thadoop_path_model = "' + hadoop_path_model + '"\n')
        file.write('\t\tpath = save_model_to_hadoop(message_' + str(mes + 1)
                   + ', model, hadoop_path_model)\n')
    elif message_i["last_stage"] == "transform":
        file.write('\t\tfrom utils.hadoop.save_to_hadoop import save_data_to_hadoop\n')
        file.write('\t\thadoop_path_data = "' + hadoop_path_data + '"\n')
        file.write('\t\tfrom modules.extend_dataframe import create_new_data\n')
        file.write('\t\tinput_col = message[0]["data"]["column_input_dictionary"]\n')
        file.write('\t\tnew_data = create_new_data(data_prediction, input_col, spark)\n')
        file.write('\t\tpath = save_data_to_hadoop(message_' + str(mes + 1)
                   + ', new_data, hadoop_path_data)\n')
    file.close()


def running_code_except(nama_main_code):
    """
    Generating exception for running code.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    """

    file = open(nama_main_code, 'a')

    # Importing modul for running
    file.write('\t# Main running code except.\n')
    file.write('\texcept Exception as e:\n')
    file.write('\t\traise e\n')
    file.close()


def write_get_result_and_main_function(message_i, nama_main_code, mes,
                                       nama_code, pipeline_type):
    """
    Writing get_result function and main function
    :param message_i: (dict) message index i
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    :param mes: (int) index of message
    :param nama_code: (string) file name of pyspark executable code being generate
    :param pipeline_type: (int)
                            0: Fit General Pipeline
                            1: Fit Cross Validator
                            2: Fit Train Validation Split
                            3: Transform
    """

    # Writing get_result function
    # Running
    running_code(message_i, nama_main_code, mes, nama_code, pipeline_type)
    # Except:
    running_code_except(nama_main_code)
    # End of the function get_result
    file = open(nama_main_code, 'a')

    if pipeline_type == 0:
        file.write('\treturn model, data_prediction, data_train, path, '
                   'time_total\n')
    elif pipeline_type == 3:
        file.write('\treturn new_data, data_train, path, time_total\n')
    else:
        file.write('\treturn data_prediction, data_train, path, time_total, '
                   'model, metric_performance\n')

    file.close()

    # Writing main function
    main_generated_and_see_result(message_i, nama_main_code)


def code_generator(nama_main_code, message, nama_code):
    """
    Main of code generator, each pipeline can generate different process.
    :param nama_main_code: (string) the file name of the code used to run
                            the executable code results (nama_code)_modul_(no).py
                            (nama_code)_main.py
    :param message: (list) contains all information is ready to generate code
    :param nama_code: (string) file name of pyspark executable code being generate
    """

    # This process for creating executable code .py of certain pipeline
    for mes in range(len(message)):
        estimator = message[mes]['estimator']
        message_i = message[mes]
        if message_i['last_stage'] == 'fit':
            # [DONE]
            if message_i['type'] == 'GeneralPipeline':
                code_generator_fit_general_pipe(estimator, mes,
                                                nama_code)
            elif message_i['type'] == 'CrossValidator':
                code_generator_fit_cv(message, estimator, mes,
                                      nama_code)
            elif message_i['type'] == 'TrainValidationSplit':
                code_generator_fit_tvs(message, estimator, mes,
                                       nama_code)
        # [DONE]
        elif message_i['last_stage'] == 'transform':
            code_generator_transform(estimator, mes, nama_code)

    # This process for creating executable main code .py for running pipeline

    # running ID Generator generator generator code
    file = open(nama_main_code, 'w')
    file.write('#!/usr/bin/env python3\n')
    file.write('# -*- coding: utf-8 -*-\n')
    file.write('# This code generated by running_executable_code.py made by: \n')
    file.write('# Author: Mujirin, mujirin@kofera.com\n')
    file.write('#=======================================================\n')
    file.close()

    # Importing main moduls
    import_modul(nama_main_code)

    # Spark and Cassandra configuration
    cluster_configuration(nama_main_code)

    # Inserting message
    insert_message(nama_main_code, message)

    # Making function for generate result
    file = open(nama_main_code, 'a')
    file.write('def get_result(message):\n')
    file.close()

    # Writing get_result and main function each pipeline type
    for mes in range(len(message)):
        message_i = message[mes]
        if message_i['last_stage'] == 'fit':
            if message_i['type'] == 'GeneralPipeline':
                # Running
                pipeline_type = 0
                write_get_result_and_main_function(
                    message_i, nama_main_code, mes, nama_code,
                    pipeline_type)
            elif message_i['type'] == 'CrossValidator':
                # Running
                pipeline_type = 1
                write_get_result_and_main_function(
                    message_i, nama_main_code, mes, nama_code,
                    pipeline_type)
            elif message_i['type'] == 'TrainValidationSplit':
                # Running
                pipeline_type = 2
                write_get_result_and_main_function(
                    message_i, nama_main_code, mes, nama_code,
                    pipeline_type)
        elif message_i['last_stage'] == 'transform':
            # Running
            pipeline_type = 3
            write_get_result_and_main_function(
                message_i, nama_main_code, mes, nama_code,
                pipeline_type)


def generate_filename(message):
    """
    Generate filename for nama code and nama main code.
    :param message: (list) contains all information is ready to generate code
    :return: (tuple)
    """

    date_now = datetime.datetime.utcnow()
    date_format = "%Y%m%d%H%M%S%f"
    date_now_formated = datetime.datetime.strftime(date_now, date_format)
    composer_id = message[0]['composer_id']
    composer_id = composer_id.replace("-", "_")
    nama_code = top_level_directory + "/code_generated/C" + date_now_formated + "_" + composer_id
    nama_main_code = nama_code + "_main.py"

    return nama_code, nama_main_code


def code_generator_and_run(message):
    """
    Generating executable code start from here (main).
    :param message: (list) contains all information is ready to generate code
    """

    try:
        # Write to cassandra and generated id
        message = write_info(message)

        # Send message as a respond: nama_model dan running_ID
        producer_response(message)

        # Generate filename of executable code
        nama_code, nama_main_code = generate_filename(message)

        # Create executable code here
        code_generator(nama_main_code, message, nama_code)

        # Print status for creating executable code
        logger.info("Executable code is successfully created with name - {0}.".format(nama_main_code))

        # Run the code generated in spark cluster
        generated_code_name = 'python ' + nama_main_code + " > " + \
                              nama_main_code + ".log"
        subprocess.Popen(generated_code_name, stdout=subprocess.PIPE,
                         shell=True)

        # Print status for running
        logger.info("Executable code starts running.")

    except Exception as er:
        n = 0
        error_msg = '[code_generator_and_run]' + str(er)
        while n < 3:
            try:
                spark_job_id = ''
                path = ''

                # Update dan produce message status to fail
                if message[0]['last_stage'] == 'fit':
                    table_name = 'fit_history'
                    last_stage_type = 0
                    update_status_and_produce_message_fail_exit(
                        message, path, table_name, spark_job_id,
                        error_msg, last_stage_type)
                elif message[0]['last_stage'] == 'transform':
                    table_name = 'transform_history'
                    last_stage_type = 1
                    update_status_and_produce_message_fail_exit(
                        message, path, table_name, spark_job_id,
                        error_msg, last_stage_type)
                else:
                    # Update status and produce message to fail
                    logger.error("last_stage undefined")
                break

            except Exception as err:
                n = n + 1
                if n == 3:
                    logger.error("[update_status_and_produce_message_fail_exit] "
                                 "had been trying " + str(n) + " times and failed.")
                    logger.exception(err)
        logger.exception(er)


if __name__ == '__main__':
    pass
